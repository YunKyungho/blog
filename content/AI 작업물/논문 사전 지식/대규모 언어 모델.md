---
title: "대규모 언어 모델"
aliases: [LLM, Large Language Model, 거대 언어 모델]
tags: [인공지능, 자연어처리, 딥러닝]
---

## 정의
대규모 언어 모델(Large Language Model, LLM)은 수십억에서 수조 개의 파라미터를 가진 심층 신경망으로, 대량의 텍스트 데이터를 학습하여 자연어를 이해하고 생성하는 인공지능 모델이다.

## 상세 설명

### 핵심 특성
- **규모(Scale)**: 수십억~수조 파라미터
- **사전학습(Pre-training)**: 대규모 코퍼스로 자기지도 학습
- **창발 능력(Emergent Abilities)**: 규모 증가 시 새로운 능력 발현
- **인컨텍스트 학습**: 프롬프트만으로 새 작업 수행

### 아키텍처
- **[[트랜스포머]]**: 대부분의 LLM이 기반
- **디코더 전용**: GPT, Llama
- **인코더-디코더**: T5, BART
- **MoE(Mixture of Experts)**: Mixtral, GPT-4

### 학습 패러다임
1. **사전학습**: 다음 토큰 예측
2. **지시 미세조정(IFT)**: 지시-응답 쌍 학습
3. **[[RLHF]]**: 인간 피드백 강화학습
4. **DPO/ORPO**: 직접 선호도 최적화

### 주요 모델
| 모델 | 개발사 | 파라미터 |
|------|--------|----------|
| GPT-4 | OpenAI | ~1.7T (추정) |
| Claude | Anthropic | - |
| Llama 3 | Meta | 8B-405B |
| Gemini | Google | - |
| DeepSeek-R1 | DeepSeek | - |

### 한계
- 할루시네이션 (환각)
- 지식 단절점
- 추론 능력 제한
- 계산 비용

## 관련 개념
- [[트랜스포머]]
- [[RLHF]]
- [[딥러닝]]
- [[멀티스텝 추론]]

## 참고
- "LLMOrbit: A Circular Taxonomy of Large Language Models" (arXiv, 2026)
- "Opening the Black Box: A Survey on Multi-Step Reasoning in LLMs" (arXiv, 2026)
