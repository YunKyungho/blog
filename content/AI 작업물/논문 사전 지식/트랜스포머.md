---
title: "트랜스포머"
aliases: [Transformer, 트랜스포머 아키텍처]
tags: [인공지능, 딥러닝, 아키텍처]
---

## 정의
트랜스포머(Transformer)는 2017년 Google이 "Attention Is All You Need" 논문에서 제안한 딥러닝 아키텍처로, Self-Attention 메커니즘을 핵심으로 하여 순차적 데이터를 병렬로 처리할 수 있는 모델이다.

## 상세 설명

### 핵심 구성 요소
1. **Self-Attention**: 입력 시퀀스 내 요소 간 관계 계산
2. **Multi-Head Attention**: 여러 관점에서 attention 수행
3. **Position Encoding**: 순서 정보 인코딩
4. **Feed-Forward Network**: 비선형 변환

### Attention 메커니즘
```
Attention(Q, K, V) = softmax(QK^T / √d_k) V
```
- Q(Query): 현재 관심 대상
- K(Key): 비교할 키들
- V(Value): 추출할 값들

### 아키텍처 유형
1. **인코더-디코더**: 번역, 요약 (T5, BART)
2. **인코더 전용**: 이해 태스크 (BERT)
3. **디코더 전용**: 생성 태스크 (GPT, Llama)

### 주요 혁신
- **순환 구조 제거**: 병렬 처리 가능
- **긴 범위 의존성**: 멀리 떨어진 토큰 관계 포착
- **전이 학습**: 사전학습-미세조정 패러다임

### 응용 분야
- **NLP**: [[대규모 언어 모델]]
- **CV**: Vision Transformer (ViT)
- **Audio**: Whisper, 음성 인식
- **Multimodal**: GPT-4V, Gemini

### 변형 및 개선
- **Efficient Transformers**: Linformer, Performer
- **MoE**: Mixture of Experts
- **MLA**: Multi-head Latent Attention

## 관련 개념
- [[대규모 언어 모델]]
- [[딥러닝]]
- [[어텐션 메커니즘]]

## 참고
- "Attention Is All You Need" (NeurIPS, 2017)
- "LLMOrbit: A Circular Taxonomy of Large Language Models" (arXiv, 2026)
