---
title: "LLMOrbit: A Circular Taxonomy of Large Language Models - From Scaling Walls to Agentic AI Systems"
arxiv: "2601.14053"
date: 2026-01-20
tags: [LLM, 인공지능, 서베이, 에이전트AI]
---

## 한줄 요약
2019-2025년 대규모 언어 모델의 발전을 8개 차원으로 분류하고, 스케일링 한계와 이를 극복하는 6가지 패러다임을 제시하는 종합 서베이.

## 핵심 내용
[[대규모 언어 모델]]은 [[트랜스포머]] 아키텍처에서 인간 수준 추론 능력을 갖춘 시스템으로 혁명적 진화를 이루었다. 이 논문은 50개 이상의 모델을 15개 조직에 걸쳐 8개 차원으로 분석한다.

### 스케일링의 세 가지 위기
1. **데이터 부족**: 2026-2028년까지 9-27조 토큰 고갈
2. **비용 폭증**: 5년간 300만→3억 달러 이상
3. **에너지 소비**: 22배 증가, 지속 불가능

### 스케일링 한계 극복 6가지 패러다임
1. **Test-Time Compute**: 추론 시 계산 증가 (o1, DeepSeek-R1)
2. **양자화**: 4-8배 압축
3. **분산 엣지 컴퓨팅**: 10배 비용 절감
4. **모델 병합**: 여러 모델 통합
5. **효율적 학습**: ORPO로 메모리 50% 절감
6. **소형 특화 모델**: Phi-4 14B가 대형 모델에 필적

### 세 가지 패러다임 전환
1. **사후 학습 개선**: [[RLHF]], GRPO, 순수 RL로 성능 향상
2. **효율성 혁명**: MoE 라우팅 18배, MLA 8배 압축
3. **민주화**: 오픈소스 Llama 3이 GPT-4 능가

### 에이전트 AI로의 진화
- 수동적 생성 → 도구 사용 에이전트
- ReAct, RAG, 멀티에이전트 시스템

## 주요 개념
- [[대규모 언어 모델]]: 수십억 파라미터의 언어 이해/생성 모델
- [[트랜스포머]]: Self-Attention 기반 아키텍처
- [[RLHF]]: 인간 피드백 강화학습
- [[AI 에이전트]]: 자율적으로 도구를 사용하는 AI 시스템

## 입문자를 위한 학습 포인트
1. **더 크다고 더 좋지 않다**: 스케일링의 물리적/경제적 한계
2. **효율성이 핵심**: 작은 모델도 잘 훈련하면 성능 발휘
3. **Test-Time Compute**: 추론 시 더 생각하면 더 정확
4. **오픈소스의 힘**: 폐쇄 모델과 경쟁하는 오픈 모델
5. **에이전트가 미래**: 단순 대화를 넘어 행동하는 AI
